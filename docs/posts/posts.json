[
  {
    "path": "posts/2022-02-03-google-summer-of-code-and-mlr3/",
    "title": "Google Summer of Code and mlr3",
    "description": "Our Org is currently thinking about participating in GSOC 2022 again.",
    "author": [
      {
        "name": "Florian Pfisterer",
        "url": "https://github.com/pfistfl"
      }
    ],
    "date": "2022-02-03",
    "categories": [],
    "contents": "\nOur Org is currently thinking about participating in GSOC 2022 again. We are considering which projects to propose and would also like to collect input. This year’s Google Summer of Code is not only directed at students but at a broader audience of people new to FOSS.\nWe have thought about the following projects:\nmlr3multioutput We would like for mlr3multioutput to include multi-label and general multi-output techniques, such as learners and prediction post-processing methods. Furthermore, some work on the evaluation of multi-output methods could drastically improve the package’s utility.\nmlr3fairness by now contains the basic infrastructure to audit algorithms for some notions of fairness. We’d like to augment mlr3fairness with several bias mitigation techniques, as well as other notions of fairness.\nQueue-based asynchronous parallelization for R mlr3 currently heavily relies on future for parallelization. Some tuning methods would profit from a different take on parallelization, where task queues are cleanly orchestrated on a central server. The goal of this project would be to develop a general-purpose parallelization package (maybe extending the future framework) for asynchronous parallelization and parallel programming.\nIf you feel like one of the projects sparks your interest, feel free to get in contact with us (mlr-org@stat.uni-muenchen.de)! Similarly, if you would like to propose your own idea/project also get in contact with us, and we will figure out if this is a fit for our organization!\n\n\n\n",
    "preview": "posts/2022-02-03-google-summer-of-code-and-mlr3/GSoC-Vertical.png",
    "last_modified": "2022-02-15T11:29:21+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-03-mlr3-package-updates-q42021/",
    "title": "mlr3 Package Updates - Q4/2021",
    "description": "This posts gives an overview by listing the recent release notes of mlr3 packages from the last quarter.",
    "author": [
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2022-01-03",
    "categories": [],
    "contents": "\nDue to the high amount of packages in the mlr3 ecosystem, it is hard to keep up with the latest changes across all packages. This posts gives an overview by listing the recent release notes of mlr3 packages from the last quarter. Note that only CRAN packages are listed here and the sort order is alphabetically.\nInterval: 2021-10-01 - 2021-12-31\nmlr3 0.13.0 - https://github.com/mlr-org/mlr3\nDescription: Machine Learning in R - Next Generation\nLearners which are capable of resuming/continuing (e.g., learner (classif|regr|surv).xgboost with hyperparameter nrounds updated) can now optionally store a stack of trained learners to be used to hotstart their training. Note that this feature is still somewhat experimental. See HotstartStack and #719.\nNew measures to score similarity of selected feature sets: sim.jaccard (Jaccard Index) and sim.phi (Phi coefficient) (#690).\npredict_newdata() now also supports DataBackend as input.\nNew function install_pkgs() to install required packages. This generic works for all objects with a packages field as well as ResampleResult and BenchmarkResult (#728).\nNew learner regr.debug for debugging.\nNew Task method $set_levels() to control how data with factor columns is returned, independent of the used DataBackend.\nMeasures now return NA if prerequisite are not met (#699). This allows to conveniently score your experiments with multiple measures having different requirements.\nFeature names may no longer contain the special character %.\nmlr3benchmark 0.1.3 - https://github.com/mlr-org/mlr3benchmark\nDescription: Analysis and Visualisation of Benchmark Experiments\nFix README.\nFix for PMCMRplus.\nmlr3db 0.4.2 - https://github.com/mlr-org/mlr3db\nDescription: Data Base Backend for ‘mlr3’\nCompatibility fixes with new duckdb version.\nmlr3learners 0.5.1- - https://github.com/mlr-org/mlr3learners\nDescription: Recommended learners for mlr3\neval_metric() is now explicitly set for xgboost learners to silence a deprecation warning.\nImproved how the added hyperparameter mtry.ratio is converted to mtry to simplify tuning.\nMultiple updates to hyperparameter sets.\nmlr3pipelines 0.4.0 - https://github.com/mlr-org/mlr3pipelines\nDescription: Preprocessing Operators and Pipelines for ‘mlr3’\nNew operator %>>!% that modifies Graphs in-place.\nNew methods chain_graphs(), concat_graphs(), Graph$chain() as alternatives for %>>% and %>>!%.\nNew methods pos() and ppls() which create lists of PipeOps/Graphs and can be seen as “plural” forms of po() and ppl().\npo() S3-method for PipeOp class that clones a PipeOp object and optionally modifies its attributes.\nGraph$add_pipeop() now clones the PipeOp being added.\nDocumentation: Clarified documentation about cloning of input arguments in several places.\nPerformance enhancements for Graph concatenation.\nMore informative error outputs.\nNew attribute graph_model GraphLearner class, which gets the trained graph.\nas_learner() S3-method for PipeOp class that turns wraps a PipeOp in a Graph and turns that into a Learner.\nChanged PipeOps:\nPipeOpHistBin: renamed ‘bins’ Param to ‘breaks’\nPipeOpImputeHist: fix handling of integer features spanning the entire represented integer range\nPipeOpImputeOOR: fix handling of integer features spanning the entire represented integer range\nPipeOpProxy: Avoid unnecessary clone\nPipeOpScale: Performance improvement\n\nmlr3proba 0.4.2 - https://github.com/mlr-org/mlr3proba\nDescription: Probabilistic Supervised Learning for ‘mlr3’\nPatch for linux.\nmlr3spatial 0.1.0 https://github.com/mlr-org/mlr3spatial\nDescription: Support for Spatial Objects Within the ‘mlr3’ Ecosystem\nInitial release.\nmlr3tuningspaces 0.0.1 - https://github.com/mlr-org/mlr3tuningspaces\nDescription: Search Spaces for Hyperparameter Tuning\nInitial release.\nmlr3viz 0.5.7 - https://github.com/mlr-org/mlr3viz\nDescription: Visualizations for ‘mlr3’\nCompatibility fix for testthat.\n\n\n\n",
    "preview": "posts/2022-01-03-mlr3-package-updates-q42021/logo.png",
    "last_modified": "2022-02-15T11:59:41+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-01-announcing-mlr3spatial/",
    "title": "Announcing mlr3spatial",
    "description": "We are happy to announce that mlr3spatial has been released on CRAN in November 2021.",
    "author": [
      {
        "name": "Marc Becker",
        "url": "https://github.com/be-marc"
      },
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2021-12-01",
    "categories": [],
    "contents": "\nWe are happy to announce that mlr3spatial has been released on CRAN in November 2021. mlr3spatial simplifies the handling of spatial objects in the mlr3 ecosystem. Before mlr3spatial, the user had to extract tabular data from spatial objects to train a model or predict spatial data.\nNow, mlr3 Tasks can directly read from spatial objects via specialized Data Backends. Such tasks can be used to train a model or to perform resampling just like any other mlr3 task. We support spatial raster objects created by the terra, raster and stars packages with DataBackendRaster. Additionally, vector data created with the sf package is handled with DataBackendVector.\nThe predict_raster() function creates spatial rasters and features with predictions from mlr3 learners. We only have to pass a task with a spatial data backend which provides the data and spatial reference. To avoid memory issues with large raster files, prediction is done in chunks. For this, the raster map is divided into multiple horizontal strips. The vertical extension of these strips is controlled by the chunksize parameter. The actual memory usage per core is a multiple of the specified chunk size. We choose a default chunk size of 200 Megabytes which should work on most consumer grade machines. If more memory is available, a larger chunk size accelerates the prediction process.\nOne after the other, the raster chunks are loaded into memory and the prediction is written to disk. Finally, the complete raster is available on disk. The learner can also make use of future-based parallelization to accelerate the predictions. The vignette on “Benchmarking parallel predictions” showcases the parallelization capabilities of mlr3spatial.\nUse Case - Landsat7 data as {stars} object\nData Preparation\n\n\n\nFirst, the TIFF files is read via stars::read_stars() and put into a DataBackendRaster. The DataBackend is then used to create a regression task with the response being layer.1.\n\n<TaskRegr:backend> (122848 x 6)\n* Target: band1\n* Properties: -\n* Features (5):\n  - dbl (5): band2, band3, band4, band5, band6\n\nFor large raster files with millions of values it helps to predict in parallel. To enable this, set learner$parallel_predict = TRUE and initiate a parallel plan via {future}, e.g. via plan(\"multisession\"). Since this is only an example, parallelization is not enabled here. Here we will use a simple regression tree as an example learner. In practice you might want to use a different learner - you can find an overview of available learners here.\n\nINFO  [11:41:05.059] 2046 \nINFO  [11:41:05.110] 2046 \n<LearnerRegrRpart:regr.rpart>\n* Model: rpart\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Type: response\n* Feature types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, selected_features, weights\n\nPrediction\nFor prediction predict_spatial() is used. It will return a raster file which contains the predictions. Users can select which R spatial format the returned raster should have. Note that by default mlr3spatial operates with SpatRaster objects from package terra. If a different output format is requested (e.g. \"stars\"), coercion is happening in the background which might take some time.\n\nINFO  [11:41:05.440] Start raster prediction \nINFO  [11:41:05.444] Prediction is executed with a chunksize of 200, 1 chunk(s) in total, 122848 values per chunk \nINFO  [11:41:05.502] 2021 \nINFO  [11:41:05.787] 2010 \nINFO  [11:41:05.795] Chunk 1 of 1 finished \nINFO  [11:41:05.808] Finished raster prediction in 0 seconds \nstars object with 2 dimensions and 1 attribute\nattribute(s):\n            Min.  1st Qu.   Median     Mean 3rd Qu.     Max.\ncadmium  62.3629 70.30233 77.01695 79.05135 89.2809 118.1429\ndimension(s):\n  from  to  offset delta                     refsys point values x/y\nx    1 349  288776  28.5 SIRGAS 2000 / UTM zone 25S FALSE   NULL [x]\ny    1 352 9120761 -28.5 SIRGAS 2000 / UTM zone 25S FALSE   NULL [y]\n\nVisualization\nFinally we can plot the predictions. The color vector is extracted from the “viridis” color palette via dput(viridis::viridis_pal()(5)) and provided to the S3 plot() call, which makes use of the S3 plot method within the stars package in this scenario.\n\n\n\n\n\n\n",
    "preview": "posts/2021-12-01-announcing-mlr3spatial/announcing-mlr3spatial_files/figure-html5/mlr3spatial-006-1.png",
    "last_modified": "2022-02-15T11:41:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-07-mlr-workshop-2021-recap/",
    "title": "mlr Workshop 2021 Recap",
    "description": "This blog post is a recap of the mlr-org workshop 2021 which took place from the 28th of September until the 10th of October.",
    "author": [
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2021-10-07",
    "categories": [],
    "contents": "\nThis blog post is a recap of the mlr-org workshop 2021 which took place from the 28th of September until the 10th of October.\nFirst of all, we would like to thank all people and organizations which made this workshop possible:\nour (19) GitHub sponsors ⭐️\nthe Statistical Learning and Data Science group of LMU Munich\ncynkra\nEssential Data Science\nIf you like our work and want to see more of such workshops where we are able to devot our full time to mlr3, consider becoming a GitHub sponsor ❤️\nmlr3\nFor our core package {mlr3} we mainly focused on maintenance and issue curation. A little new goodie relates to scoring and aggregating: one can now conduct more complex benchmarks using different evaluation metics.\nmlr3pipelines\n{mlr3pipelines} has also seen a lot of maintenance in the first place. In addition, we did some code profiling and were able to improve the performance a bit by reducing the overhead when cloning Paramsets and learners. This comes with the new %>>!% operator which concatenetes partial Graphs in place and is essentially carrying the memory and speed improvements from above.\nA new sugar function pos() now exists, making it possible to initialize multiple pipe operators more easily.\nWe also started working on adding more sugar to {paradox} (e.g. trafos) which should make life easier in {mlr3pipelines} in the long run.\nLast, we overwhauled some error messages internally to make them more descriptive for users.\nTuning\nmlr3tuning / bbotk / mlr3mbo / mlr3hyperband\nmlr3hyperband\nOne focus with respect to tuning was on “hot starting” learners. This means that learners can save their tuning state at an arbitrary point and can be retrained later on starting from this point. This means that tuning can be done in decoupled chunks instead of one big task. This is especially helpful for certain tuning methods like “hyperband”, which is why we have listed this feature in this section.\nProposal\nbbotk\nAnother improvement for {bbotk} and all tuning packages was the support for asynchronous evaluations of hyperparameter configurations. Before, one had to wait until all hyperband configurations were evaluated to propose a new point. Now, new points can be evaluated at any time, making more efficient use of parallelization. This also avoids the issue of waiting on a few slow workers with an ineffecient configuriation that takes a long time to finish.\nmlr3mbo\nWe made great progress in finalizing {mlr3mbo}. {mlr3mbo} is a flexible Bayesian optimization toolkit and much more modular than its predecessor {mlrMBO}. Most of the functionality {mlrMBO} offers is already integrated, and we are looking forward to a CRAN release in the near future - in the meantime make sure to check out the package on GitHub.\nmlr3learners\nFor our curated set of learners we updated some ParamSet to the most recent versions and ensured that our custom “Paramtest”, i.e. the heuristic how we validate the ParamSets of mlr3learners against their upstream packages, works again smoothly.\nmlr3fairness\n{mlr3} now allows for bias auditing and debiasing through mlr3fairness for arbitrary learners. The package contains Fairness Metrics, Debiasing Strategies and Visualizations for arbitrary models trained through {mlr3}. We plan a to realease on CRAN soon, but the package is already usable. If you wanna learn more, check out the tutorials for the individual components:\nFairness Metrics\nDebiasing Methods\nFairness Visualizations\nmlr3fairness example plotsmlr3viz\nWe created a new ggplot2 theme theme_mlr3() which is being applied by default to all plots created with {mlr3viz}, i.e. all autoplot() methods in mlr3. This theme is heavily inspired by ggpubr::theme_pubr(). In addition to the theme, all mlr3viz plots now use the viridis color palette by default. Last, we have added some information how users can easily apply theming changes to the plots returned by autoplot().\ntheme_mlr3() example plotmlr3spatial\nmlr3spatial is a new package for spatial backends able to handle {sf}, {stars} and {terra} objects. It is capable of performing predictions on spatial raster objects in parallel using the internal mlr3 parallelization heuristic (which uses the {future} framework under the hood). Together with the {mlr3spatiotempcv} package it extends the support for spatial machine learning in mlr3.\nmlr-org meta\nRoadmap\nWe have createad a Roadmap of upcoming packages and planned features across the whole mlr3 ecosystem. This roadmap aims both for internal organization and giving external people an idea of what is upcoming. The roadmap is quite new and not yet fully operational at the time this blog post got released - however, we encourage to look at it from time to time as we try to keep it up-to-date.\nmlr3 wiki\nWe have updated and reorganized our mlr3 wiki. It now has a better sidebar appearance for easier navigation and we have updated the sections with respect to their content.\nmlr3book\nText and structure improvements (mainly for the basics chapter)\nNew introduction\n\n\n\n",
    "preview": "posts/2021-10-07-mlr-workshop-2021-recap/mlr3fairness-example.png",
    "last_modified": "2022-02-15T11:59:27+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-mlr3-package-updates-q32021/",
    "title": "mlr3 Package Updates - Q3/2021",
    "description": "This posts gives an overview by listing the recent release notes of mlr3 packages from the last quarter.",
    "author": [
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2021-09-29",
    "categories": [],
    "contents": "\nDue to the high amount of packages in the mlr3 ecosystem, it is hard to keep up with the latest changes across all packages. This post tries to tackle this issue by listing all release notes of the packages most recent releases in the last quarter. Note that only CRAN packages are listed here and the sort order is alphabetically.\nInterval: 2021-07-01 - 2021-10-01\nbbotk 0.4.0 - https://github.com/mlr-org/bbotk\nDescription: Black-Box Optimization Toolkit\nAdds non dominated sorting with hypervolume contribution to Archive.\nAllows empty search space and domain.\nExtended TerminatorEvals with an additional hyperparameter k to define the budget depending on the dimension of the search space.\nAdds bb_optimize() function for quick optimization.\nAdds OptimizerIrace from irace package.\nmlr3 0.12.0 - https://github.com/mlr-org/mlr3\nDescription: Machine Learning in R - Next Generation\nNew method to assign labels to columns in tasks: Task$label(). These will be used in visualizations in the future.\nNew method to add stratification variables: Task$add_strata().\nNew helper function partition() to split a task into a training and test set.\nNew standardized getter loglik() for class Learner.\nNew measures \"aic\" and \"bic\" to compute the Akaike Information Criterion or the Bayesian Information Criterion, respectively.\nNew Resampling method: ResamplingCustomCV. Creates a custom resampling split based on the levels of a user-provided factor variable.\nNew argument encapsulate for resample() and benchmark() to conveniently enable encapsulation and also set the fallback learner to the featureless learner. This is simply for convenience, configuring each learner individually is still possible and allows a more fine-grained control (#634, #642).\nNew field parallel_predict for Learner to enable parallel predictions via the future backend. This currently is only enabled while calling the $predict() or $predict_newdata methods and is disabled during resample() and benchmark() where you have other means to parallelize.\nDeprecated public (and already documented as internal) field $data in ResampleResult and BenchmarkResult to simplify the API and avoid confusion. The converter as.data.table() can be used instead to access the internal data.\nMeasures now have formal hyperparameters. A popular example where this is required is the F1 score, now implemented with customizable beta.\nChanged default of argument ordered in Task$data() from TRUE to FALSE.\nmlr3cluster 0.1.2 - https://github.com/mlr-org/mlr3cluster\nDescription: Cluster Extension for ‘mlr3’\nAdd Hclust\ntest and doc hclust\nAdd within sum of squares measure\nadd doc wss\ncode factor adaptions\nmlr3filters 0.4.2 - https://github.com/mlr-org/mlr3filters\nDescription: Filter Based Feature Selection for ‘mlr3’\nFixes an issue where argument nfeat was not passed down to {praznik} filters (#97)\nmlr3learners 0.5.1 - https://github.com/mlr-org/mlr3learners\nDescription: Recommended Learners for ‘mlr3’\nImproved how the added hyperparameter mtry.ratio is converted to mtry to simplify tuning.\nmlr3pipelines 0.3.4 - https://github.com/mlr-org/mlr3pipelines\nDescription: Preprocessing Operators and Pipelines for ‘mlr3’\nStability: PipeOps don’t crash when they have python/reticulate hyperparameter values.\nDocumentation: Titles of PipeOp documentation articles reworked.\nmlr3spatiotempcv 1.0.0 - https://github.com/mlr-org/mlr3spatiotempcv\nDescription: Spatiotemporal Resampling Methods for ‘mlr3’\nBreaking\nautoplot(): removed argument crs. The CRS is now inferred from the supplied Task. Setting a different CRS than the task might lead to spurious issues and the initial idea of changing the CRS for plotting to have proper axes labeling does not apply (anymore) (#144)\nFeatures\nAdded autoplot() support for ResamplingCustomCV (#140)\nBug fixes\n\"spcv_block\": Assert error if folds > 2 when selection = \"checkerboard\" (#150)\nFixed row duplication when creating TaskRegrST tasks from sf objects (#152)\nMiscellaneous\nUpgrade tests to {vdiffr} 1.0.0\nAdd {rgdal} to suggests and required it in \"spcv_block\" since it is required in {blockCV} >= 2.1.4 and {sf} >= 1.0\nmlr3tuning 0.9.0 - https://github.com/mlr-org/mlr3tuning\nDescription: Tuning for ‘mlr3’\nAdds AutoTuner$base_learner() method to extract the base learner from nested learner objects.\ntune() supports multi-criteria tuning.\nAllows empty search space.\nAdds TunerIrace from irace package.\nextract_inner_tuning_archives() helper function to extract inner tuning archives.\nRemoves ArchiveTuning$extended_archive() method. The mlr3::ResampleResults are joined automatically by as.data.table.TuningArchive() and extract_inner_tuning_archives().\nmlr3verse 0.2.3 - https://github.com/mlr-org/mlr3verse\nDescription: Easily Install and Load the ‘mlr3’ Package Family\nUpdated reexports.\nmlr3viz 0.5.6 - https://github.com/mlr-org/mlr3viz\nDescription: Visualizations for ‘mlr3’\nCompatibility fix for mlr3tuning.\nFixed position of labels in barplot for PredictionClassif.\nmlr3viz 0.5.5\nFixed another bug for ROC- and Precision-recall-curves (#79).\n\n\n\n",
    "preview": "posts/2021-09-29-mlr3-package-updates-q32021/logo.png",
    "last_modified": "2022-02-15T11:59:16+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-20-the-cross-validation-trainpredict-misunderstanding/",
    "title": "The Cross-Validation - Train/Predict Misunderstanding",
    "description": "ver the past years I've seen multiple posts on Stackoverflow and our GitHub issues which suffer from a conceptual misunderstanding: cross-validation (CV) vs. train/predict.",
    "author": [
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2020-12-20",
    "categories": [],
    "contents": "\nIntroduction\nOver the past years I’ve seen multiple posts on Stackoverflow and our GitHub issues which suffer from a conceptual misunderstanding: cross-validation (CV) vs. train/predict.\nBecause train/predict is an essential part of cross-validation, the point might not be so obvious. I’ll try to make it more clear by providing some exemplary questions:\n“I’ve done cross-validation. How do I decided which model to use for prediction?”\n“I’ve used the resample() function. How do I decided which hyperparameters are best?”\n“I’ve benchmarked some algorithms. How do I find the most important features?”\nAll of these questions have a common problem: users trying to extract something out of the CV to work with it afterwards. They want to use the “best model” or find the “most important features” or similar. The thoughtful reader might already infer by now that doing so is probably problematic.\nAnd yes, it is. One should not try to extract a single component (be it a model, a set of hyperparameters or a set of predictors) out of a CV.\nWhat happens in a CV, stays in a CV\nHere’s why:\nEvery model fit and evaluation in a CV happens on a subset of the main dataset.\nThe dataset is split by a specific method (e.g. randomly) into pre-selected partitions (e.g. 5).\n(Optional) Optimal hyperparameters are searched and feature selection is performed (in another inner CV cycle)\nThe model is fit on the training set and predicts on the test set\nThe performance of this prediction is evaluated (because “truth” (= test set) is known)\nThis is done multiple times. Every time, the dataset is different. Every time, different hyperparameters are found. Every time - ok you got it by now.\nNone of these training/test set combinations represent overall “the best” choice - they only operate in their specific data split setting. There is also no way to find a model (or similar) within a CV with respect to these criteria.\nThe main reason for this is that in all cases the fitted model was trained on only a subset of the data available. This was done to evaluate the performance on a subset of the data - because “truth” is known for the hold back data. Otherwise there would be no need to hide precious data from model fitting.\nTrain & Predict\nThe main purpose of fitting a model is make predictions with it. For this, you want to use all available data to fit the most robust model possible. And this is exactly what you should do: take all your data, optimize your hyperparameters, eventually conduct feature selection, and then fit the model.\nYes, do it again - by using the train() and predict() functions (and their tuning wrappers/pipelines) directly. Do not use resample() or benchmark() - these are for CV purposes!\nThen, take this one model and predict into unknown space. In this scenario, you cannot know how good your predictions will be because there is no “truth” to evaluate against. But this is perfectly fine. This is why a CV was done (beforehand): to have a somewhat unbiased estimate of how your model will perform under different conditions. You can also analyse the hyperparameters or evaluate the results of a feature selection from this model1.\nBe careful though: your final model which you fit and predicted into unknown space might exactly have this performance - but it might also be completely off. You will never find out unless you eventually collect data at some point which you can compare against the predictions made by your model. You can only state that, with a given uncertainty, your model will have a performance of X when predicting.\nSummary\nTo make if fully clear again: CV and train/predict are two separate things. Think of them as two different buckets with no relation to each other.\nCV is done to get an estimate of a model’s performance.\nTrain/predict is done to create the final predictions (which your boss might use to make some decisions on).\nCV is used to explain the performance of your fitted model (which is a single fit of the chosen algorithm on all of your data points).\n\nWhile it might be ok to look at feature selections of single model fits, it is usually recommended to fit multiple models and look at the average variable important outcomes - e.g. by doing a permutation-based features selection.↩︎\n",
    "preview": "posts/2020-12-20-the-cross-validation-trainpredict-misunderstanding/theme-mlr3-example.png",
    "last_modified": "2022-02-15T11:59:06+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-12-announcing-mlr3spatiotempcv/",
    "title": "Announcing mlr3spatiotempcv",
    "description": "We are happy to announce that mlr3spatial has been released on CRAN in November 2020.",
    "author": [
      {
        "name": "Patrick Schratz",
        "url": "https://github.com/pat-s"
      }
    ],
    "date": "2020-11-12",
    "categories": [],
    "contents": "\nWe are happy to announce that a new extension package has joined the CRAN family of mlr3 packages. mlr3spatiotempcv was in the works for more than a year and adds spatiotemporal resampling methods to the mlr3 ecosystem.\nSuch dedicated resampling methods make it possible to retrieve biased-reduced performance estimates in cross-validation scenarios when working with spatial, temporal, or spatiotemporal datasets. mlr3spatiotempcv does not implement new methods but rather attempts to collect existing methods.\nSo far, applying such methods in both R and the mlr ecosystem was not particularly easy since they were spread across various R packages. Usually, every R package uses a slightly different syntax for the required objects and the returned results. This not only leads to an inconvenient single-use experience but is also unpractical when working in an overarching ecosystem such as mlr3.\nWe hope that with the release of this package users are now able to seamlessly work with spatiotemporal data in mlr3. Please file issues and suggestions in the issues pane of the package.\nFor a quick and rather technical introduction please see the “Get Started” vignette. For more detailed information and a detailed walk-through, see the “Spatiotemporal Analysis” section in the mlr3book.\nTo finish with something visual, a simple example which showcases the visualization capabilities of mlr3spatiotempcv for different partitioning methods (random (non-spatial) partitioning (Fig.1) vs. k-means based partitioning (spatial) (Fig. 2)):\n\n\n\n\n\n\n",
    "preview": "posts/2020-11-12-announcing-mlr3spatiotempcv/announcing-mlr3spatiotempcv_files/figure-html5/mlr3spatiotempcv-vis-example-1.png",
    "last_modified": "2022-02-16T16:36:42+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-26-introducing-mlr3cluster-cluster-analysis-package/",
    "title": "Introducing mlr3cluster: Cluster Analysis Package",
    "description": "Tired of learning to use multiple packages to access clustering algorithms?",
    "author": [
      {
        "name": "Damir Pulatov",
        "url": "https://github.com/damirpolat"
      }
    ],
    "date": "2020-08-26",
    "categories": [],
    "contents": "\nTired of learning to use multiple packages to access clustering algorithms?\nUsing different packages makes it difficult to compare the performance of clusterers?\nIt would be great to have just one package that makes interfacing all things clustering easy?\nmlr3cluster to the rescue!\nmlr3cluster is a cluster analysis extention package within the mlr3 ecosystem. It is a successsor of mlr’s cluster capabilities in spirit and functionality.\nIn order to understand the following introduction and tutorial you need to be familiar with R6 and mlr3 basics. See chapters 1-2 of the mlr3book if you need a refresher.\nInstallation\nTo install the package, run the following code chunk:\n\n\ninstall.packages(\"mlr3cluster\")\n\n\n\nGetting Started\nAssuming you know all the basics and you’ve installed the package, here’s an example on how to perform k-means clustering on a classic usarrests data set:\n\n<PredictionClust> for 50 observations:\n    row_ids partition\n          1         1\n          2         1\n          3         1\n---                  \n         48         2\n         49         2\n         50         2\n\nIntegrated Learners\nWhat built-in clusterers does the package come with? Here is a list of integrated learners:\n\n [1] \"clust.agnes\"        \"clust.ap\"           \"clust.cmeans\"      \n [4] \"clust.cobweb\"       \"clust.dbscan\"       \"clust.diana\"       \n [7] \"clust.em\"           \"clust.fanny\"        \"clust.featureless\" \n[10] \"clust.ff\"           \"clust.hclust\"       \"clust.kkmeans\"     \n[13] \"clust.kmeans\"       \"clust.MBatchKMeans\" \"clust.meanshift\"   \n[16] \"clust.pam\"          \"clust.SimpleKMeans\" \"clust.xmeans\"      \n\nThe library contains all the basic types of clusterers: partitional, hierarchial, density-based and fuzzy. Below is a detailed list of all the learners.\nID\nLearner\nPackage\nclust.agnes\nAgglomerative Hierarchical Clustering\ncluster\nclust.cmeans\nFuzzy C-Means Clustering\ne1071\nclust.dbscan\nDensity-based Clustering\ndbscan\nclust.diana\nDivisive Hierarchical Clustering\ncluster\nclust.fanny\nFuzzy Clustering\ncluster\nclust.featureless\nSimple Featureless Clustering\nmlr3cluster\nclust.kmeans\nK-Means Clustering\nstats\nclust.pam\nClustering Around Medoids\ncluster\nclust.xmeans\nK-Means with Automatic Determination of k\nRWeka\nIntegrated Measures\nList of integrated cluster measures:\n\n[1] \"clust.ch\"         \"clust.db\"         \"clust.dunn\"      \n[4] \"clust.silhouette\" \"clust.wss\"       \n\nBelow is a detailed list of all the integrated learners.\nID\nMeasure\nPackage\nclust.db\nDavies-Bouldin Cluster Separation\nclusterCrit\nclust.dunn\nDunn index\nclusterCrit\nclust.ch\nCalinski Harabasz Pseudo F-Statistic\nclusterCrit\nclust.silhouette\nRousseeuw’s Silhouette Quality Index\nclusterCrit\nIntegrated Tasks\nThere is only one built-in Task in the package:\n\n<TaskClust:usarrests> (50 x 4)\n* Target: -\n* Properties: -\n* Features (4):\n  - int (2): Assault, UrbanPop\n  - dbl (2): Murder, Rape\n\nAs you can see, the biggest difference in clustering tasks as compared to the rest of the tasks in mlr3 is the absense of the Target column.\nHyperparameters\nSetting hyperparameters for clusterers is as easy as setting parameters for any other mlr3 learner:\n\n<ParamSet>\n          id    class lower upper nlevels       default value\n1:   centers ParamUty    NA    NA     Inf             2     2\n2:  iter.max ParamInt     1   Inf     Inf            10      \n3: algorithm ParamFct    NA    NA       4 Hartigan-Wong      \n4:    nstart ParamInt     1   Inf     Inf             1      \n5:     trace ParamInt     0   Inf     Inf             0      \n\nTrain and Predict\nThe “train” method is simply creating a model with cluster assignments for data, while the “predict” method’s functionality varies depending on the clusterer in question. Read the each learner’s documentation for details.\nFor example, the kmeans learner’s predict method uses clue::cl_predict which performs cluster assignments for new data by looking at the “closest” neighbors of the new observations.\nFollowing the example from the previous section:\n\n<PredictionClust> for 10 observations:\n    row_ids partition\n          1         1\n          6         1\n         11         2\n---                  \n         38         2\n         46         2\n         47         2\n\nBenchmarking and Evaluation\nTo assess the quality of any machine learning experiment, you need to choose an evaluation metric that makes the most sense. Let’s design an experiment that will allow you to compare the performance of three different clusteres on the same task. The mlr3 library provides benchmarking functionality that lets you create such experiments.\n\n              task                  learner              resampling\n1: <TaskClust[45]> <LearnerClustKMeans[37]> <ResamplingHoldout[19]>\n2: <TaskClust[45]>    <LearnerClustPAM[37]> <ResamplingHoldout[19]>\n3: <TaskClust[45]> <LearnerClustCMeans[37]> <ResamplingHoldout[19]>\nINFO  [09:55:04.812] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [09:55:05.173] [mlr3] Applying learner 'clust.pam' on task 'usarrests' (iter 1/1) \nINFO  [09:55:05.639] [mlr3] Applying learner 'clust.cmeans' on task 'usarrests' (iter 1/1) \nINFO  [09:55:05.773] [mlr3] Applying learner 'clust.kmeans' on task 'usarrests' (iter 1/1) \nINFO  [09:55:05.830] [mlr3] Finished benchmark \n   nr      resample_result   task_id   learner_id resampling_id iters\n1:  1 <ResampleResult[22]> usarrests clust.kmeans       holdout     1\n2:  2 <ResampleResult[22]> usarrests    clust.pam       holdout     1\n3:  3 <ResampleResult[22]> usarrests clust.cmeans       holdout     1\n   clust.silhouette\n1:              NaN\n2:        0.4760311\n3:              NaN\n\nVisualization\nHow do you visualize clustering tasks and results? The mlr3viz package (version >= 0.40) now provides that functionality.\n\n\ninstall.packages(\"mlr3viz\")\n\n\n\n\n\nlibrary(mlr3viz)\n\ntask = mlr_tasks$get(\"usarrests\")\nlearner = mlr_learners$get(\"clust.kmeans\")\nlearner$param_set$values = list(centers = 3L)\nlearner$train(task)\npreds = learner$predict(task)\n\n# Task visualization\nautoplot(task)\n\n\n\n# Pairs plot with cluster assignments\nautoplot(preds, task)\n\n\n\n# Silhouette plot with mean silhouette value as reference line\nautoplot(preds, task, type = \"sil\")\n\n\n\n# Performing PCA on task data and showing cluster assignments\nautoplot(preds, task, type = \"pca\")\n\n\n\n\nKeep in mind that mlr3viz::autoplot also provides more options depending on the kind of plots you’re interested in. For example, to draw borders around clusters, provide appropriate parameters from ggfortify::autoplot.kmeans:\n\n\nautoplot(preds, task, type = \"pca\", frame = TRUE)\n\n\n\n\nYou can also easily visualize dendrograms:\n\n\ntask = mlr_tasks$get(\"usarrests\")\nlearner = mlr_learners$get(\"clust.agnes\")\nlearner$train(task)\n\n# Simple dendrogram\nautoplot(learner)\n\n\n\n# More advanced options from `factoextra::fviz_dend`\nautoplot(learner,\n  k = learner$param_set$values$k, rect_fill = TRUE,\n  rect = TRUE, rect_border = c(\"red\", \"cyan\"))\n\n\n\n\nFurther Development\nIf you have any issues with the package or would like to request a new feature, feel free to open an issue here.\nAcknowledgements\nI would like to thank the following people for their help and guidance: Michel Lang, Lars Kotthoff, Martin Binder, Patrick Schratz, Bernd Bischl.\n\n\n\n",
    "preview": "posts/2020-08-26-introducing-mlr3cluster-cluster-analysis-package/introducing-mlr3cluster-cluster-analysis-package_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-02-16T09:55:44+01:00",
    "input_file": {}
  }
]
